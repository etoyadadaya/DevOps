## Container orchestration:

1. В чем преимущества Kubernetes как платформы?
   <details>
      <summary> Ответ: </summary>

   ***В числе ее плюсов*** — возможность проектирования несколькими группами разработчиков одновременно, обновления и масштабирования каждого компонента отдельно. Такой подход позволяет создать более стабильное приложение и не допускает, чтобы одна ошибка все разрушила.

   ***Программное управление релизами:***

   ***Kubernetes*** позволяет программно управлять релизами на серверах под своим управлением:

   * накатить релиз одной кнопкой;

   * быстро откатить релиз;

   * сделать a/b тестирование;

   * выкатывать постепенно (по процентам), следя за показателями мониторинга, чтобы быстро обнаружить возможные ошибки в новой версии;

   * автоматически увеличить или уменьшить размер кластера, то есть добавить или убрать ноды в зависимости от нагрузки;

   * следить, чтобы было ровно заданное количество инстансов приложения, например — быстро доводить их до нужного числа при потере части инстансов из-за какого-либо сбоя.

   ***Автоматическая отказоустойчивость:***

   Для приложения на физическом сервере нужно продумывать схему его восстановления при сбоях:

    * если приложение работает на одном сервере и он выходит из строя — надо думать, как организовать его автоматическое развертывание в новом месте;

    * если приложение работает в нескольких копиях параллельно, чтобы при падении одной всегда оставалась рабочая копия, — нужно настраивать автоматическую балансировку нагрузки.

   ***В Kubernetes эти проблемы решены автоматически:*** он сам распределяет приложения так, чтобы они продолжили работу в случае сбоя на конкретном физическом сервере, а также с учетом нагрузки на них. Например, если зона отказа — дата-центр, то он будет распределять приложение по разным дата-центрам. Таким образом, потеря дата-центра может не сказаться на приложении.

   ***Kubernetes — это автоматизация процессов:***

   ***Приложения в k8s*** выкатываются и тестируются без участия администраторов. Разработчик пишет сценарий, а дальше происходит облачная магия. Так что в идеальном мире Kubernetes вся операционная поддержка работы софта лежит на плечах программистов, а администраторы следят, чтобы стабильно работал слой облачной инфраструктуры — то есть, сам Kubernetes.

   Подробнее: https://mcs.mail.ru/blog/put-k-kubernetes-i-ego-preimushchestva-dlya-razrabotki
  </details>


2. Что такое control plane и из каких компонентов состоит?
   <details>
      <summary> Ответ: </summary>

   Компонент Control Plane запускает процессы контроллера.

   Вполне логично, что каждый контроллер в свою очередь представляет собой отдельный процесс, и для упрощения все такие процессы скомпилированы в один двоичный файл и выполняются в одном процессе.

    Эти контроллеры включают:
 
    * Контроллер узла (Node Controller): уведомляет и реагирует на сбои узла.
 
    * Контроллер репликации (Replication Controller): поддерживает правильное количество подов для каждого объекта контроллера репликации в системе.
    
    * Контроллер конечных точек (Endpoints Controller): заполняет объект конечных точек (Endpoints), то есть связывает сервисы (Services) и поды (Pods).
    
    * Контроллеры учетных записей и токенов (Account & Token Controllers): создают стандартные учетные записи и токены доступа API для новых пространств имен.

   Подробнее: https://kubernetes.io/ru/docs/concepts/overview/components/
    </details>


3. Какие CNI вы использовали, и чем они отличаются?
   <details>
      <summary> Ответ: </summary>

   Container Networking Interface (CNI) — сетевой интерфейс и стандарт для Linux-контейнеров.

   CNI (Container Networking Interface). Его задача — обеспечить всё необходимое для стандартизированного управления сетевыми интерфейсами в Linux-контейнерах и гибкого расширения сетевых возможностей.

   Подробнее: Пока нет инфы
    </details>


4. Чем отличается managed Kubernetes от self-deployed?
   <details>
      <summary> Ответ: </summary>

    ***Self-hosted:***

   ***Инфраструктурная команда***. Люди, которые будут заниматься железом, сетью и дата-центром. Они есть в компании, когда вы держите свои собственные сервера или даже целый дата-центр. Но чаще всего сейчас компании арендуют сами серверы, а за железо отвечает хостер. В таком случае нужно заниматься только мониторингом серверов и заказом новых в случае необходимости. Отдельная команда для этого не нужна — справятся и обычные администраторы.

   ***Администраторы Kubernetes***. Собственно те, кто работают с кластером: разворачивают его, настраивают, обслуживают, расширяют и мониторят нагрузку. Именно им можно поручить инфраструктуру, которая держится на арендованных серверах.
   
    Также они будут интегрировать кластер с внешними сервисами, например, системами хранения данных или балансировщиками нагрузки. И следить, чтобы работал централизованный сбор логов, системы бэкапов, системы мониторинга, CNI, CSI, DNS и сontrol plane.

   ***Команда автоматизации***. Эти ребята занимаются DevOps — помогают разработчикам деплоить приложения в Kubernetes, настраивают CI/CD, совместно с командой администраторов k8s не допускают деплоя манифестов, которые могут привести к проблемам. Например, деплойментов, у которых количество реплик подов равно единице и которые совершенно не отказоустойчивы.

   ***Managed***:

   ***Команда по работе с облаком***. Команда инфраструктурщиков есть у хостера — именно они обеспечивают работоспособность «железа». Но в компании нужны люди, которые понимают, как облако работает и как с ним обращаться. Без них не обойтись, пускай большинство облаков и говорят, что они простые и дружелюбные с пользователями. Это поможет избежать ситуаций, когда вам в конце месяца приходит счет на десяток тысяч у.е. за мощности, которые вы случайно заказали и не использовали.

   ***Администраторы Kubernetes***. Да, устанавливать Kubernetes, настраивать его и масштабировать не понадобится. Но задачи по поддержанию мониторинга, сбору логов, валидации манифестов и пресеканию неправильных деплоев остаются. Их будет меньше, чем на собственных серверах, но команда по автоматизации с ними не справится — понадобятся отдельные люди.

   ***Команда автоматизации***. DevOps вам нужен в любом случае, и облачный провайдер его не обеспечит.

   ***Ответ на главный вопрос: что же выбрать?***
   
    Как мы и говорили в начале, универсального ответа на этот вопрос нет — решать всё-таки придётся самостоятельно. Но если у вас небольшой бизнес, низкие нагрузки и нет отдельной команды для Kubernetes — можно выбирать managed-решение. Так вы сосредоточитесь на деплое, и всё будет хорошо.
   
    Как только подрастёте, нагрузки станут высокими и вместо трёх микросервисов у вас будет три десятка, можно будет набрать специалистов и построить своё self-hosted решение, настроенное точно под вас.

   Подробнее: https://habr.com/ru/company/southbridge/blog/661825/ (Очень годно)
    </details>


5. Как можно контролировать размещение подов в кластере? (taints/tolerations, affinities, topologies etc.)
   <details>
      <summary> Ответ: </summary>

   ***Зачем вообще нужно привязывать поды к определенным узлам?*** Это может быть связано с производительностью, безопасностью или надежность. Например – pod может требовать доступ к специфическому железу (видеокарты и ML-ускорители для задач машинного обучения, аппаратные криптоускорители). Это может быть продиктовано безопасностью: критические части проекта будут размещаться на машинах, где физически не может быть ничего, кроме них. Это снижает шансы на то, что удачный взлом, скажем, сервиса регистраций раскроет данные о платежах. Некоторые стандарты безопасности (включая PCI DSS) имеют даже требования к физической безопасности серверов – датчики вскрытия, пломбы на корпусках, запрет на доступ. Отдельная удобная особенность – tier-инг. Нагрузку в кластере можно разделить на “важную” и “не очень”. Под важную выделять мощные современные машины с резервированием PSU, горячей замены дисков и памяти, под “не очень” – соскрести какой-нибудь хлам. В облаках это делается даже проще за счет spot instances. Такие инстансы дешевле (порой радикально), но их работу никто не гарантирует – инстанс может отключится в любой момент (вместо него появится новый). Это вызовет пересоздание POD-ов, но для чего-то маловажного это, может – и не страшно совсем.

   ***Taints and Tolerations:***

   ***Taints*** – это NodeAffinity наоборот. Если nodeAffinity говорит scheduler-у, где он должен размещать pod-ы, то taint говорит, где pod-ы размещать нельзя.

   ***nodeAffinity:***

   Несмотря на простоту и эффективность механизма nodeSelector – механизм это прямолинейный и не особенно гибкий. Авторы kubernetes предлагают более мощный, гибкий (а так же – сложный и неудобный) механизм – nodeAffinity. Язык описания nodeAffinity предлагает несколько мощных возомжностей:

    * логические операторы для выбора условия размещения – IN (размещать на одной из нод с разными метками) или AND (размещать на нодах, имеющих обе метки сразу).

    * можно выбирать политики размещения pod-ов относительно друг друга: например – запретить экземплярам кэша оказываться на одной физической машине или требовать размещение приложения вместе с экземпляром кеша на одном физическом узле.

   Подробнее: https://prudnitskiy.pro/post/2021-01-15-k8s-pod-distribution/
    </details>


6. Скейлинг кластера. Cluster autoscaler vs HPA vs VPA? Как сделать zero-downtime node decommission/cluster upgrade? PDB? Lifecycle hooks?
   <details>
      <summary> Ответ: </summary>

   ***Модуль горизонтального автомасштабирования (HPA):***

   Как следует из названия, HPA масштабирует количество реплик pod'ов. В качестве триггеров для изменения количества реплик большинство девопсов используют нагрузку на процессор и память. Однако можно масштабировать систему на основе пользовательских метрик, их сочетания или даже внешних метрик.

   ***Вертикальное автомасштабирование (VPA):***

   Вертикальное автомасштабирование (VPA) выделяет больше (или меньше) времени процессора или памяти для существующих pod'ов. Подходит для pod'ов с сохранением состояния (stateful) или без него (stateless), но в основном предназначено для stateful-сервисов. Впрочем, вы можете применить VPA и для модулей без сохранения состояния, если нужно автоматически скорректировать объем изначально выделенных ресурсов. 

   VPA также реагирует на события OOM (out of memory, недостаточно памяти). Для изменения процессорного времени и объема памяти требуется перезапуск pod'ов. При перезапуске VPA соблюдает бюджет распределения (pods distribution budget, PDB), чтобы гарантировать минимально необходимое количество модулей.

   Вы можете установить минимальный и максимальный объем ресурсов для каждого модуля. Так, можно ограничить максимальный объем выделяемой памяти пределом в 8 ГБ. Это полезно, если текущие узлы точно не могут выделить более 8 ГБ памяти на контейнер. Подробные спецификации и механизм работы описаны в официальном вики VPA.

   Кроме того, у VPA есть интересная функция рекомендаций (VPA Recommender). Она отслеживает использование ресурсов и события OOM всех модулей, чтобы предложить новые значения памяти и процессорного времени на основе интеллектуального алгоритма с учетом исторических метрик. Также есть API-интерфейс, который принимает дескриптор pod и отдает предлагаемые значения ресурсов.

   Стоит отметить, что VPA Recommender не отслеживает «лимит» ресурсов. Это может привести к тому, что модуль монополизирует ресурсы внутри узлов. Лучше установить предельное значение на уровне пространства имен, чтобы избежать огромного расхода памяти или процессорного времени.

   ***Автомасштабирование кластера (Cluster Autoscaler, CA):***

   Автомасштабирование кластера (Cluster Autoscaler, CA) изменяет количество узлов, исходя из количества ожидающих модулей pod. Система периодически проверяет наличие ожидающих модулей — и увеличивает размер кластера, если требуется больше ресурсов и если кластер не выходит за пределы установленных лимитов. CA взаимодействует с поставщиком облачных услуг, запрашивает у него дополнительные узлы или освобождает бездействующие. Первая общедоступная версия CA была представлена в Kubernetes 1.8.

   ***Как сделать zero-downtime node decommission/cluster upgrade:*** https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-upgrading-your-clusters-with-zero-downtime

   ***POD:*** https://habr.com/ru/company/nixys/blog/490680/

   ***Lifecycle hooks:*** https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/

   Подробнее: https://habr.com/ru/company/vk/blog/484344/
    </details>


7. Какие способы для внешнего доступа к кластеру? ingress, node port, port-forward и т. д.
   <details>
      <summary> Ответ: </summary>

   ***NodePort:***

   ***Сервис NodePort*** — самый примитивный способ направить внешний трафик в сервис. NodePort, как следует из названия, открывает указанный порт для всех Nodes (виртуальных машин), и трафик на этот порт перенаправляется сервису.

   ***Ingress:***

    В отличие от приведенных примеров, Ingress сам по себе не сервис. Он стоит перед несколькими сервисами и действует как «интеллектуальный маршрутизатор» или точка вхождения в кластер.

   ***Port-Forward:***

   ***Переадресация портов*** — это часть NAT, которая перенаправляет IP-адрес и номер порта одной системы на другую систему. Он имеет дело с одним IP-адресом и портом и часто используется между хостами в Интернете и отдельным хостом в локальной сети (LAN) или демилитаризованной зоне (DMZ).

   Подробнее: https://habr.com/ru/company/southbridge/blog/358824/
    </details>


8. С каким PID запускается процесс в контейнере?
   <details>
      <summary> Ответ: </summary>

   ***Основной процесс запущен с PID 1***

   Подробнее: https://it-lux.ru/docker-entrypoint-pid-1/
    </details>


9. Что лучше использовать для изоляции окружения – Vagrant или Docker?
   <details>
      <summary> Ответ: </summary>

   Советую связку из Vagrant + Ansible. Разница будет в команде инициализации (docker-compose up -d против vagrant up), но изменять окружение будет гораздо комфортней, гибче и быстрей, а также Вы будете лишены ненужных прослоек и ограничений которые есть в Docker.

   Подробнее: https://ru.stackoverflow.com/questions/573492/docker-vs-vagrant (вся инфа)
    </details>


10. Какой инструмент оркестрирования контейнеров использовали? (Swarm, Kubernetes, Openshift, Rancher и т. д.)
   <details>
      <summary> Ответ: </summary>

Подробнее: https://habr.com/ru/company/d2cio/blog/349138/ (про инструменты)
</details>


11. Что происходит в Kubernetes после запуска kubectl (API, ReplicaSet Controller, storage back-end, scheduler, kubelet, worker node, pod)?
   <details>
      <summary> Ответ: </summary>

Подробнее: https://habr.com/ru/company/flant/blog/342822/
</details>


12. Какая разница между pod и контейнером в K8s?
   <details>
      <summary> Ответ: </summary>

   Pods (Поды):
   
   Это абстрактный объект Kubernetes, представляющий собой «обертку» для одного или группы контейнеров. Контейнеры в поде запускаются и работают вместе, имеют общие сетевые ресурсы и хранилище. Kubernetes не управляет контейнерами напрямую, он собирает их в поды и работает с ими.

Подробнее: https://blog.skillfactory.ru/glossary/kubernetes/
</details>


13. Как мы можем сделать любой микросервис, работающий на K8s, доступным из внешней среды?
   <details>
      <summary> Ответ: </summary>

Подробнее: https://ipsoftware.ru/posts-cloud/k8s-3-services/ (хороший материал)
</details>
